{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acfd4bd",
   "metadata": {},
   "source": [
    "# Smart Study Coach — Data Exploration, Cleaning & Recommendation\n",
    "\n",
    "Inhoud:\n",
    "1. **Ruwe Analyse** — korte analyse van de ruwe data, counts (unique tags, locations), en error rate per kolom.\n",
    "2. **Cleaning & Preprocessing** — the cleaning pipeline with explanations and functions. Produces a cleaned CSV.\n",
    "3. **Clean Analyse** — analyse van de schoonen data, counts (unique tags, locations), en error rate per kolom.\n",
    "4. **NLP & Recommendation Engine** — text cleaning, TF-IDF vectorization, cosine similarity and a `recommend()` helper.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966ad21-c554-4dd6-b6c1-9720ce6b09f6",
   "metadata": {},
   "source": [
    "# Business Understanding & Data Collection\n",
    "\n",
    "**Probleemstelling**\n",
    "\n",
    "Binnen het huidige studieprogramma ervaren studenten vaak moeilijkheden bij het kiezen van een passende vrije keuzemodule (VKM). Het aanbod is groot, de beschikbare informatie is verspreid, en de begeleiding bij het maken van keuzes is beperkt. Hierdoor nemen studenten soms beslissingen die niet optimaal aansluiten bij hun interesses, waarden of loopbaandoelen.\n",
    "\n",
    "Het doel van dit project is daarom om een Smart Study Coach te ontwikkelen: een AI-toepassing die studenten ondersteunt bij het maken van correcte keuze van een vrij keuze module. Deze toepassing zal het studentprofiel analyseren en gepersonaliseerde aanbevelingen doen op basis van de overeenkomsten met de beschikbare modules.\n",
    "\n",
    "**Maatschappelijke relevantie**\n",
    "\n",
    "Het keuzeproces van studenten heeft een directe invloed op motivatie, studiesucces en welzijn. Door studenten beter te begeleiden, kan deze toepassing bijdragen aan:\n",
    "1. Hogere studiebetrokkenheid en motivatie.\n",
    "2. Minder studievertraging of verkeerde keuze.\n",
    "3. Een beter aansluitend studiepad richting persoonlijke en professionele doelen.\n",
    "\n",
    "De Smart Study Coach draagt bij aan de bredere maatschappelijke trend van verantwoorde AI in het onderwijs, waarin technologie wordt ingezet om gelijke kansen en persoonlijke ontwikkeling te bevorderen.\n",
    "\n",
    "**Ethiek en privacy (EU AI Act 2025 / AVG)**\n",
    "\n",
    "Bij het ontwikkelen van een AI-systeem voor studieadvies moet goed worden opgelet dat alles volgens de ethische en wettelijke regels gebeurt. De volgende principes, die direct of indirect voortkomen uit de EU AI Act 2025 en de AVG, sluiten goed aan op ons AI-systeem:\n",
    "1. Transparantie & uitlegbaarheid: De aanbevelingen van de Smart Study Coach moeten begrijpelijk en uitlegbaar zijn, zodat studenten weten waarom een module wordt voorgesteld.\n",
    "2. Privacy by Design: Persoonlijke gegevens van studenten (zoals interesses of waarden) worden uitsluitend gebruikt voor het aanbevelingsdoel en veilig opgeslagen volgens de AVG-richtlijnen.\n",
    "3. Data-minimisatie: Er wordt enkel data verzameld die strikt noodzakelijk is voor het functioneren van het model.\n",
    "4. Menselijke controle: De uiteindelijke keuze blijft altijd bij de student; de AI dient als ondersteunend instrument, niet als beslisser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f969c3a",
   "metadata": {},
   "source": [
    "## Part 1 — Ruwe Analyse\n",
    "We beginnen met het lade van de dataset en dan korte analyse :\n",
    "- aantal unique tags (module_tags)\n",
    "- aantal modules per locatie `location`\n",
    "- kolom counts of empty / weird / ntb values (pre-cleaning)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cbb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Load dataset\n",
    "raw_path = \"Uitgebreide_VKM_dataset.csv\"\n",
    "df_raw = pd.read_csv(raw_path)\n",
    "print(f\"Loaded dataset with {len(df_raw)} rows and {len(df_raw.columns)} columns\")\n",
    "\n",
    "# Setup Stopwords for Tags (Dutch + English)\n",
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('dutch'))\n",
    "# Add specific noise words if necessary\n",
    "extra_noise = {\"ntb\", \"nan\", \"null\", \"none\", \"'\", \"['\", \"']\"} \n",
    "stop_words.update(extra_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_values = [\"\", \"nan\", \"none\", \"null\", \"[]\"]\n",
    "weird_values = [\n",
    "    \"nvt\", \"volgt\", \"nader te bepalen\", \"nog niet bekend\",\n",
    "    \"nadert te bepalen\", \"nog te formuleren\", \"tbd\", \"n.n.b.\", \"navragen\", \"['ntb']\"\n",
    "]\n",
    "\n",
    "def is_empty(value):\n",
    "    if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "        return True\n",
    "    if isinstance(value, str) and value.strip() == \"\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_weird(value):\n",
    "    if not isinstance(value, str):\n",
    "        return False\n",
    "    val = value.lower().strip()\n",
    "    return any(w in val for w in weird_values)\n",
    "\n",
    "def is_ntb(value):\n",
    "    return isinstance(value, str) and value.strip().lower() == \"ntb\"\n",
    "\n",
    "def analyze_dataframe_simple(df_in):\n",
    "    analysis = []\n",
    "    for col in df_in.columns:\n",
    "        total = len(df_in[col])\n",
    "        empty_count = df_in[col].apply(is_empty).sum()\n",
    "        weird_count = df_in[col].apply(is_weird).sum()\n",
    "        ntb_count = df_in[col].apply(is_ntb).sum()\n",
    "        general_error_count = empty_count + weird_count + ntb_count\n",
    "        general_error_percent = round((general_error_count / total) * 100, 2)\n",
    "        analysis.append({\n",
    "            \"column\": col,\n",
    "            \"empty_values\": int(empty_count),\n",
    "            \"empty_%\": round((empty_count / total) * 100, 2),\n",
    "            \"weird_values\": int(weird_count),\n",
    "            \"weird_%\": round((weird_count / total) * 100, 2),\n",
    "            \"ntb\": int(ntb_count),\n",
    "            \"ntb_%\": round((ntb_count / total) * 100, 2),\n",
    "            \"general_error_total\": int(general_error_count),\n",
    "            \"general_error_%\": general_error_percent\n",
    "        })\n",
    "    analysis_df = pd.DataFrame(analysis).sort_values(by=\"general_error_%\", ascending=False)\n",
    "    return analysis_df\n",
    "\n",
    "print(\"Column analysis (pre-cleaning):\")\n",
    "col_analysis_pre = analyze_dataframe_simple(df_raw)\n",
    "display(col_analysis_pre)\n",
    "\n",
    "def visualize_data(df_in, title_stage=\"Pre-Cleaning\"):\n",
    "    \"\"\"\n",
    "    Generates bar charts for Locations and Top Tags.\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # --- 1. Location Analysis ---\n",
    "    if 'location' in df_in.columns:\n",
    "        loc_counts = df_in['location'].fillna('ntb').astype(str).str.lower().value_counts().head(10)\n",
    "        sns.barplot(x=loc_counts.values, y=loc_counts.index, ax=axes[0], palette=\"viridis\")\n",
    "        axes[0].set_title(f\"Top 10 Locations ({title_stage})\")\n",
    "        axes[0].set_xlabel(\"Count\")\n",
    "    \n",
    "    # --- 2. Tag Analysis ---\n",
    "    if 'module_tags' in df_in.columns:\n",
    "        tags_series = df_in['module_tags'].fillna('')\n",
    "        all_tags = []\n",
    "        \n",
    "        for t in tags_series.astype(str):\n",
    "            # Clean string representation of list to actual words\n",
    "            # Remove brackets, quotes, and split by comma\n",
    "            cleaned = re.sub(r\"[\\[\\]']\", \"\", t) \n",
    "            parts = cleaned.split(',')\n",
    "            for p in parts:\n",
    "                p = p.strip().lower()\n",
    "                # Basic filtering for the plot (ignoring raw empty strings)\n",
    "                if p and len(p) > 1:\n",
    "                    all_tags.append(p)\n",
    "                    \n",
    "        # Count and plot\n",
    "        tag_counts = Counter(all_tags).most_common(15)\n",
    "        if tag_counts:\n",
    "            tags_df = pd.DataFrame(tag_counts, columns=['Tag', 'Count'])\n",
    "            sns.barplot(x='Count', y='Tag', data=tags_df, ax=axes[1], palette=\"magma\")\n",
    "            axes[1].set_title(f\"Top 15 Tags ({title_stage})\")\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, \"No Tags Found\", ha='center')\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run Visualization on Raw Data\n",
    "print(\"Generating Pre-Cleaning Visualizations...\")\n",
    "visualize_data(df_raw, title_stage=\"Before Cleaning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce969b",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusie\n",
    "In de tags staat nog veel leeg of stopwoorden deze moeten eruit. Ook is er nog veel noise data in de shortdescription, description en content. deze moeten gevuld worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "202c5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load dataset\n",
    "raw_path = \"Uitgebreide_VKM_dataset.csv\"\n",
    "df_raw = pd.read_csv(raw_path)\n",
    "print(f\"Loaded dataset with {len(df_raw)} rows and {len(df_raw.columns)} columns\")\n",
    "\n",
    "# Quick peek\n",
    "display(df_raw.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1a68b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_values = [\"\", \"nan\", \"none\", \"null\", \"[]\"]\n",
    "weird_values = [\n",
    "    \"nvt\", \"volgt\", \"ntb\", \"nader te bepalen\", \"nog niet bekend\",\n",
    "    \"nadert te bepalen\", \"nog te formuleren\", \"tbd\", \"n.n.b.\", \"navragen\", \"['ntb']\"\n",
    "]\n",
    "\n",
    "def is_empty(value):\n",
    "    if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "        return True\n",
    "    if isinstance(value, str) and value.strip() == \"\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_weird(value):\n",
    "    if not isinstance(value, str):\n",
    "        return False\n",
    "    val = value.lower().strip()\n",
    "    return any(w in val for w in weird_values)\n",
    "\n",
    "def is_ntb(value):\n",
    "    return isinstance(value, str) and value.strip().lower() == \"ntb\"\n",
    "\n",
    "def analyze_dataframe_simple(df_in):\n",
    "    analysis = []\n",
    "    for col in df_in.columns:\n",
    "        total = len(df_in[col])\n",
    "        empty_count = df_in[col].apply(is_empty).sum()\n",
    "        weird_count = df_in[col].apply(is_weird).sum()\n",
    "        ntb_count = df_in[col].apply(is_ntb).sum()\n",
    "        general_error_count = empty_count + weird_count\n",
    "        general_error_percent = round((general_error_count / total) * 100, 2)\n",
    "        analysis.append({\n",
    "            \"column\": col,\n",
    "            \"empty_values\": int(empty_count),\n",
    "            \"empty_%\": round((empty_count / total) * 100, 2),\n",
    "            \"weird_values\": int(weird_count),\n",
    "            \"weird_%\": round((weird_count / total) * 100, 2),\n",
    "            \"ntb\": int(ntb_count),\n",
    "            \"ntb_%\": round((ntb_count / total) * 100, 2),\n",
    "            \"general_error_total\": int(general_error_count),\n",
    "            \"general_error_%\": general_error_percent\n",
    "        })\n",
    "        # Unique tags analysis (module_tags column assumed)\n",
    "    if 'module_tags' in df_in.columns:\n",
    "        # Split tag strings by common separators and count unique tags\n",
    "        tags_series = df_in['module_tags'].fillna('')\n",
    "        tag_counter = Counter()\n",
    "        for t in tags_series.astype(str):\n",
    "            # consider comma, semicolon, pipe and slash as separators\n",
    "            parts = re.split(r\"[,;/\\\\|]+\", t)\n",
    "            for p in parts:\n",
    "                p = p.strip().lower()\n",
    "                if p and p not in empty_values and p not in weird_values:\n",
    "                    tag_counter[p] += 1\n",
    "        print(f\"Found {len(tag_counter)} unique tags\")\n",
    "        top_tags = tag_counter.most_common(30)\n",
    "        display(pd.DataFrame(top_tags, columns=['tag','count']))\n",
    "    else:\n",
    "        print(\"No 'module_tags' column found in the dataset\")\n",
    "\n",
    "    # Location distribution\n",
    "    if 'location' in df_in.columns:\n",
    "        loc_counts = df_in['location'].fillna('ntb').astype(str).str.lower().value_counts()\n",
    "        display(loc_counts.head(50))\n",
    "    else:\n",
    "        print(\"No 'location' column found in the dataset\")\n",
    "    analysis_df = pd.DataFrame(analysis).sort_values(by=\"general_error_%\", ascending=False)\n",
    "    return analysis_df\n",
    "\n",
    "print(\"Column analysis (pre-cleaning):\")\n",
    "col_analysis_pre = analyze_dataframe_simple(df_raw)\n",
    "display(col_analysis_pre)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb698952",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 — Cleaning & Preprocessing\n",
    "We perform the cleaning steps with explanations. The strategy is:\n",
    "1. Drop irrelevant columns (colors) if present.\n",
    "2. Normalize values to string, lowercase and trim.\n",
    "3. Replace literal empty indicators with `ntb` (Not To Be Determined).\n",
    "4. Apply a safe regex replacement for known weird phrases (only if cell content exactly matches one of them).\n",
    "5. Smart-fill `shortdescription` from `description` and `content` where available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528550b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "cols_to_drop = [\"Rood\", \"Groen\", \"Blauw\", \"Geel\"]\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "\n",
    "# Convert to string (safe for TF-IDF later) and normalize\n",
    "df = df.fillna('')\n",
    "for col in df.columns:\n",
    "    # Cast to string for consistent processing\n",
    "    df[col] = df[col].astype(str)\n",
    "    df[col] = df[col].str.lower().str.strip()\n",
    "\n",
    "# Replace explicit empty-like strings with 'ntb'\n",
    "for val in empty_values:\n",
    "    df.replace(val, 'ntb', inplace=True)\n",
    "\n",
    "# Safe regex for weird_values: only replace if the entire cell equals the weird phrase\n",
    "safe_pattern = r'^\\s*(' + '|'.join([re.escape(v) for v in weird_values]) + r')\\s*$'\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].replace(to_replace=safe_pattern, value='ntb', regex=True)\n",
    "\n",
    "# Specific Tag Cleaning (Stopword Removal)\n",
    "def clean_tags_column(tag_string):\n",
    "    if tag_string == 'ntb': return 'ntb'\n",
    "    \n",
    "    # 1. Remove list characters like [ ] ' \"\n",
    "    clean_str = re.sub(r\"[\\[\\]'\\\"]\", \"\", tag_string)\n",
    "    \n",
    "    # 2. Split by comma\n",
    "    tags = clean_str.split(',')\n",
    "    \n",
    "    valid_tags = []\n",
    "    for tag in tags:\n",
    "        tag = tag.strip().lower()\n",
    "        # 3. Filter: Must not be a stopword, must be > 1 char, must not be numeric\n",
    "        if tag and tag not in stop_words and len(tag) > 1 and not tag.isdigit():\n",
    "            valid_tags.append(tag)\n",
    "            \n",
    "    # Return as a clean comma-separated string (easier for reading) \n",
    "    # or keep as list string if preferred. Here we join them.\n",
    "    return \", \".join(valid_tags) if valid_tags else \"ntb\"\n",
    "\n",
    "print(\"Removing stopwords from module_tags...\")\n",
    "df['module_tags'] = df['module_tags'].apply(clean_tags_column)\n",
    "\n",
    "# Smart fill for shortdescription\n",
    "def fill_short_smart(row):\n",
    "    short = row.get('shortdescription', 'ntb')\n",
    "    if short and short != 'ntb':\n",
    "        return short\n",
    "    desc = row.get('description', 'ntb')\n",
    "    content = row.get('content', 'ntb')\n",
    "    valid_desc = desc and desc != 'ntb'\n",
    "    valid_content = content and content != 'ntb'\n",
    "    if valid_desc and valid_content:\n",
    "        if desc == content:\n",
    "            return desc\n",
    "        return f\"{desc} {content}\"\n",
    "    if valid_desc:\n",
    "        return desc\n",
    "    if valid_content:\n",
    "        return content\n",
    "    return 'ntb'\n",
    "\n",
    "if 'shortdescription' in df.columns:\n",
    "    print(\"Filling shortdescription using description/content where needed...\")\n",
    "    df['shortdescription'] = df.apply(fill_short_smart, axis=1)\n",
    "else:\n",
    "    print(\"No shortdescription column found; skipping smart fill.\")\n",
    "\n",
    "print('\\nAnalysis after cleaning:')\n",
    "col_analysis_post = analyze_dataframe_simple(df)\n",
    "display(col_analysis_post)\n",
    "\n",
    "print(\"Generating Post-Cleaning Visualizations...\")\n",
    "visualize_data(df, title_stage=\"Before Cleaning\")\n",
    "\n",
    "\n",
    "# Save cleaned file\n",
    "out_path = 'Uitgebreide_VKM_dataset_zonder_weird_data.csv'\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"Cleaned file written to: {out_path}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c889a0",
   "metadata": {},
   "source": [
    "## Conclusie\n",
    "Na het opschonen en filteren zijn er geen stopworden meer in de tags. en ook de vertaling is duidelijk te zien aangezien tags zoals onderzoek nu vaker voor komen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdedc34d",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 — NLP & Recommendation Engine\n",
    "We clean the text for NLP, vectorize using TF-IDF and compute cosine similarity. Explanations follow the cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306eeef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure NLTK resources are available (the notebook will attempt to download if missing)\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    WordNetLemmatizer()\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except Exception:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Prepare stopwords\n",
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('dutch'))\n",
    "lemmatizer_en = WordNetLemmatizer()\n",
    "stemmer_nl = SnowballStemmer('dutch')\n",
    "\n",
    "def detect_language(text):\n",
    "    dutch_keywords = [\"de\",\"het\",\"een\",\"en\",\"je\",\"jij\",\"wij\",\"zijn\",\"module\",\"leren\",\"opleiding\"]\n",
    "    english_keywords = [\"the\",\"a\",\"an\",\"and\",\"is\",\"are\",\"course\",\"learn\"]\n",
    "    text_low = text.lower()\n",
    "    nl_score = sum(1 for w in dutch_keywords if w in text_low)\n",
    "    en_score = sum(1 for w in english_keywords if w in text_low)\n",
    "    return 'nl' if nl_score >= en_score else 'en'\n",
    "\n",
    "def clean_text_nlp(text):\n",
    "    if not isinstance(text, str) or text.strip() == '' or text.lower() in ['ntb','tbd','nader te bepalen']:\n",
    "        return 'ntb'\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Záéíóúàèçäëïöüñ\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    lang = detect_language(text)\n",
    "    words = [w for w in text.split() if w not in stop_words]\n",
    "    if lang == 'nl':\n",
    "        words = [stemmer_nl.stem(w) for w in words]\n",
    "    else:\n",
    "        words = [lemmatizer_en.lemmatize(w) for w in words]\n",
    "    return ' '.join(words) if words else 'ntb'\n",
    "\n",
    "print('Applying NLP cleaning to shortdescription and description...')\n",
    "df['shortdescription'] = df['shortdescription'].apply(clean_text_nlp) if 'shortdescription' in df.columns else ''\n",
    "df['description'] = df['description'].apply(clean_text_nlp) if 'description' in df.columns else ''\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        df[col] = df[col].str.lower()\n",
    "\n",
    "df['combined_text'] = (\n",
    "    df.get('name', pd.Series(['']*len(df))).astype(str) + ' ' +\n",
    "    df.get('shortdescription', pd.Series(['']*len(df))).astype(str) + ' ' +\n",
    "    df.get('module_tags', pd.Series(['']*len(df))).astype(str) + ' ' +\n",
    "    df.get('location', pd.Series(['']*len(df))).astype(str)\n",
    ")\n",
    "\n",
    "combined_stopwords = list(set(stopwords.words('dutch') + stopwords.words('english')))\n",
    "vectorizer = TfidfVectorizer(stop_words=combined_stopwords, min_df=1)\n",
    "matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "print(f\"TF-IDF matrix shape: {matrix.shape}\")\n",
    "similarities = cosine_similarity(matrix)\n",
    "similarity_df = pd.DataFrame(similarities, index=df.get('name', pd.Series(range(len(df)))), columns=df.get('name', pd.Series(range(len(df)))))\n",
    "\n",
    "def recommend(module_name, similarity_df, top_n=5):\n",
    "    if module_name not in similarity_df.index:\n",
    "        print('Module not found. Showing top items from dataset index:')\n",
    "        print(list(similarity_df.index)[:10])\n",
    "        return []\n",
    "    recs = similarity_df.loc[module_name].sort_values(ascending=False)[1:top_n+1]\n",
    "    results = [(name, float(score)) for name, score in recs.items()]\n",
    "    print(f\"Recommendations for '{module_name}':\")\n",
    "    for name, score in results:\n",
    "        print(f\"- {name} (score={score:.3f})\")\n",
    "    return results\n",
    "\n",
    "# Example\n",
    "example_name = df['name'].iloc[0] if 'name' in df.columns else None\n",
    "if example_name is not None:\n",
    "    recommend(example_name, similarity_df)\n",
    "else:\n",
    "    print('No name column found to demonstrate recommendations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47fd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_profile_vector(\n",
    "    favorite_tags, liked_modules, skipped_modules, df, vectorizer, combined_text_col='combined_text'\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a single vector for the user based on their input, using the existing TF-IDF Vectorizer.\n",
    "    \n",
    "    Args:\n",
    "        favorite_tags (list): List of user's favorite tags (e.g., ['python', 'ai', 'cloud']).\n",
    "        liked_modules (list): List of names of modules the user liked.\n",
    "        skipped_modules (list): List of names of modules the user skipped (currently used for exclusion/filtering).\n",
    "        df (pd.DataFrame): The main DataFrame containing module data.\n",
    "        vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
    "        combined_text_col (str): The column in df that contains the pre-processed combined text.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr.csr_matrix: The TF-IDF vector for the user profile.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Combine positive text sources\n",
    "    positive_profile_text = []\n",
    "    \n",
    "    # A. Add Favorite Tags (clean them first)\n",
    "    cleaned_tags = [tag.lower().strip() for tag in favorite_tags if tag.strip()]\n",
    "    positive_profile_text.extend(cleaned_tags)\n",
    "    \n",
    "    # B. Add combined_text of Liked Modules\n",
    "    liked_module_names = set(liked_modules)\n",
    "    \n",
    "    # Find the rows for liked modules and exclude skipped ones from being liked (just in case)\n",
    "    valid_liked_indices = df[df['name'].isin(liked_module_names)].index\n",
    "    \n",
    "    # We should filter out skipped modules just in case they're also in 'liked_modules'\n",
    "    skipped_module_names = set(skipped_modules)\n",
    "    \n",
    "    for idx in valid_liked_indices:\n",
    "        module_name = df.loc[idx, 'name']\n",
    "        if module_name not in skipped_module_names:\n",
    "            # Append the already combined and cleaned text\n",
    "            positive_profile_text.append(df.loc[idx, combined_text_col])\n",
    "    \n",
    "    # The user profile is a single string of all collected content\n",
    "    user_text = ' '.join(positive_profile_text)\n",
    "    \n",
    "    # 2. Vectorize the user's profile text using the *fitted* vectorizer\n",
    "    # We pass it as a list with a single element\n",
    "    user_vector = vectorizer.transform([user_text])\n",
    "    \n",
    "    return user_vector\n",
    "\n",
    "def recommend_to_user(\n",
    "    favorite_tags, liked_modules, skipped_modules, df, vectorizer, matrix, top_n=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates module recommendations based on a user's explicit preferences.\n",
    "    \n",
    "    Args:\n",
    "        favorite_tags (list): User's favorite tags.\n",
    "        liked_modules (list): Names of modules the user liked.\n",
    "        skipped_modules (list): Names of modules the user skipped.\n",
    "        df (pd.DataFrame): The main DataFrame.\n",
    "        vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
    "        matrix (sparse matrix): The TF-IDF matrix for all modules.\n",
    "        top_n (int): Number of recommendations to return.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tuples (module_name, score).\n",
    "    \"\"\"\n",
    "    # 1. Create the user vector\n",
    "    user_vector = create_user_profile_vector(\n",
    "        favorite_tags, liked_modules, skipped_modules, df, vectorizer\n",
    "    )\n",
    "    \n",
    "    # 2. Compute similarity between the user vector and all module vectors\n",
    "    # This returns a 1xN array of scores\n",
    "    user_similarities = cosine_similarity(user_vector, matrix).flatten()\n",
    "    \n",
    "    # 3. Get the indices of the top N modules\n",
    "    # np.argsort returns indices that would sort an array\n",
    "    # [::-1] reverses the order for descending (most similar first)\n",
    "    # [0:top_n] takes the top N indices\n",
    "    top_indices = user_similarities.argsort()[::-1][0:top_n]\n",
    "    \n",
    "    results = []\n",
    "    recommended_names = set()\n",
    "    \n",
    "    # Get the names of the modules to exclude (liked, skipped)\n",
    "    exclude_names = set(liked_modules) | set(skipped_modules)\n",
    "    \n",
    "    print(\"\\nUser Profile:\")\n",
    "    print(f\"- Tags: {favorite_tags}\")\n",
    "    print(f\"- Liked: {liked_modules}\")\n",
    "    print(f\"- Skipped: {skipped_modules}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Top {top_n} Recommendations for User:\")\n",
    "    \n",
    "    # We iterate through the top indices and skip any that the user has already interacted with\n",
    "    # We may need to check more than top_n indices to find top_n *new* modules.\n",
    "    max_search_idx = min(len(user_similarities), len(user_similarities) + 10) \n",
    "    \n",
    "    for i in user_similarities.argsort()[::-1]:\n",
    "        module_name = df.loc[i, 'name']\n",
    "        score = user_similarities[i]\n",
    "        \n",
    "        # Exclude modules the user has already liked or skipped\n",
    "        if module_name not in exclude_names and module_name not in recommended_names:\n",
    "            recommended_names.add(module_name)\n",
    "            results.append((module_name, score))\n",
    "            print(f\"- {module_name} (score={score:.3f})\")\n",
    "            \n",
    "        if len(results) >= top_n:\n",
    "            break\n",
    "    \n",
    "    if not results:\n",
    "        print(\"Could not find any new recommendations based on the profile.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Demonstration of New Function ---\n",
    "\n",
    "# 1. Get some example names from the dataset for demonstration\n",
    "example_module_1 = df['name'].iloc[1] if len(df) > 1 else \"Module A\"\n",
    "example_module_2 = df['name'].iloc[2] if len(df) > 2 else \"Module B\"\n",
    "example_module_3 = df['name'].iloc[3] if len(df) > 3 else \"Module C\"\n",
    "example_module_4 = df['name'].iloc[4] if len(df) > 4 else \"Module D\"\n",
    "\n",
    "# 2. Define a hypothetical user's input (this is what comes from the site)\n",
    "user_favorite_tags = ['data science', 'ai', 'amsterdam']\n",
    "user_liked_modules = [example_module_1, example_module_2]\n",
    "user_skipped_modules = [example_module_3]\n",
    "\n",
    "# 3. Run the new user-based recommendation function\n",
    "if 'name' in df.columns:\n",
    "    recommend_to_user(\n",
    "        favorite_tags=user_favorite_tags,\n",
    "        liked_modules=user_liked_modules,\n",
    "        skipped_modules=user_skipped_modules,\n",
    "        df=df,\n",
    "        vectorizer=vectorizer,\n",
    "        matrix=matrix,\n",
    "        top_n=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03df90d",
   "metadata": {},
   "source": [
    "### Resultaat\n",
    "- User input wordt verwerkt door verschillende data als input te vragen zoals de favoriete modules en de skipped modules. en de gelikte tags\n",
    "- Recomend service geeft top 5 terug met score en bredenering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace5dab",
   "metadata": {},
   "source": [
    "---\n",
    "### Notes & Next steps\n",
    "- The notebook saves a cleaned CSV at `/mnt/data/Uitgebreide_VKM_dataset_zonder_weird_data.csv`.\n",
    "- You can tweak TF-IDF `min_df` and stopwords to improve recommendations.\n",
    "- If you'd like plots (e.g. bar charts for top tags or locations), tell me and I will add them into the Exploration section.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
